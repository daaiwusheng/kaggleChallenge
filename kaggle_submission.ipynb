{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### kaggle要求notebook文件且不能联网，所以需要用到的未原装包要自行放到kaggle dataset里，再用sys.path.append()添加路径","metadata":{}},{"cell_type":"code","source":"import sys\nsys.path.append('../input/labnn/labml-nn')\nsys.path.append('../input/labnn/helpers')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nimport collections\nfrom PIL import Image\nfrom torchvision import transforms\nfrom torchvision import transforms as T\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\n\nfrom labml_helpers.module import Module\nfrom labml_nn.utils import clone_module_list","metadata":{"execution":{"iopub.status.busy":"2021-12-19T14:36:48.353723Z","iopub.execute_input":"2021-12-19T14:36:48.354501Z","iopub.status.idle":"2021-12-19T14:36:49.989619Z","shell.execute_reply.started":"2021-12-19T14:36:48.354451Z","shell.execute_reply":"2021-12-19T14:36:49.988858Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"WIDTH = 704\nHEIGHT = 520\n\nBATCH_SIZE = 32\n\nPATCH_SIZE = 64\n\nTEST_CSV = \"../input/sartorius-cell-instance-segmentation/sample_submission.csv\"\nTEST_PATH = \"../input/sartorius-cell-instance-segmentation/test\" # guanhui\nRESULT_DIR = \"./\" \nLOG_PATH = './exp.log'\n\nPRETRAIN_MODEL = '../input/cell-pretrain-model/convmixer_iou061.pth'","metadata":{"execution":{"iopub.status.busy":"2021-12-19T14:36:50.361180Z","iopub.execute_input":"2021-12-19T14:36:50.361869Z","iopub.status.idle":"2021-12-19T14:36:50.366899Z","shell.execute_reply.started":"2021-12-19T14:36:50.361830Z","shell.execute_reply":"2021-12-19T14:36:50.365885Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## DATASET","metadata":{}},{"cell_type":"code","source":"\nclass CellDataset_submit(Dataset):\n    def __init__(self, image_dir, df, transforms=None, patch_size=16):\n        self.transforms = transforms\n        self.image_dir = image_dir\n        self.df = df\n        self.patch_size = patch_size\n        self.height = HEIGHT\n        self.width = WIDTH\n\n        self.row_sum = self.height // self.patch_size +1\n        self.col_sum = self.width // self.patch_size\n        self.patch_num = self.row_sum * self.col_sum\n\n        self.image_info = collections.defaultdict(dict)\n#         temp_df_11 = self.df.groupby('id')['annotation'].agg(lambda x: list(x)).reset_index()\n\n#         train_df, val_df = train_test_split(temp_df_11, test_size=0.2, random_state=42)\n        temp_df = self.df\n\n        ## 将id, path, annotation转成字典\n        ii = 0\n        for index, row in temp_df.iterrows():\n            self.image_info[ii] = {\n                'image_id': row['id'],\n                'image_path': os.path.join(self.image_dir, row['id'] + '.png'),\n            }\n            ii += 1\n\n    def __getitem__(self, idx):\n        ''' Get the image and the target'''\n        img_idx = idx // self.patch_num\n        patch_idx = idx % self.patch_num\n\n        img_path = self.image_info[img_idx][\"image_path\"]\n        img_name = self.image_info[img_idx]['image_id']\n        img = Image.open(img_path)\n\n        info = self.image_info[img_idx]\n        img = np.array(img)\n\n        # 计算patch位置\n        cur_row = patch_idx // self.col_sum\n        cur_col = patch_idx % self.col_sum\n        \n        if cur_row == 8:\n            start_row = self.height - 64\n            end_row = start_row + self.patch_size\n            start_col = cur_col * self.patch_size\n            end_col = start_col + self.patch_size\n        else:\n            start_row = cur_row * self.patch_size\n            end_row = start_row + self.patch_size\n            start_col = cur_col * self.patch_size\n            end_col = start_col + self.patch_size\n        \n        # 切片\n        image_clip = img[start_row:end_row, start_col:end_col]\n\n        transform = transforms.Compose([\n            transforms.ToTensor()\n        ])\n        image_clip = transform(image_clip)\n\n        image_clip = torch.as_tensor(image_clip, dtype=torch.float)\n        \n        return image_clip,img_name\n\n    def __len__(self):\n        return len(self.image_info) * (self.height // self.patch_size +1) * (704 // self.patch_size)\n","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-12-19T15:24:51.402612Z","iopub.execute_input":"2021-12-19T15:24:51.402910Z","iopub.status.idle":"2021-12-19T15:24:51.419245Z","shell.execute_reply.started":"2021-12-19T15:24:51.402876Z","shell.execute_reply":"2021-12-19T15:24:51.418210Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"class ConvMixerLayer(Module):\n    \"\"\"\n    <a id=\"ConvMixerLayer\"></a>\n    ## ConvMixer layer\n    This is a single ConvMixer layer. The model will have a series of these.\n    \"\"\"\n\n    def __init__(self, d_model: int, kernel_size: int):\n        \"\"\"\n        * `d_model` is the number of channels in patch embeddings, $h$\n        * `kernel_size` is the size of the kernel of spatial convolution, $k$\n        \"\"\"\n        super().__init__()\n        # Depth-wise convolution is separate convolution for each channel.\n        # We do this with a convolution layer with the number of groups equal to the number of channels.\n        # So that each channel is it's own group.\n        self.depth_wise_conv = nn.Conv2d(d_model, d_model,\n                                         kernel_size=kernel_size,\n                                         groups=d_model,\n                                         padding=(kernel_size - 1) // 2)\n        # Activation after depth-wise convolution\n        self.act1 = nn.GELU()\n        # Normalization after depth-wise convolution\n        self.norm1 = nn.BatchNorm2d(d_model)\n\n        # Point-wise convolution is a $1 \\times 1$ convolution.\n        # i.e. a linear transformation of patch embeddings\n        self.point_wise_conv = nn.Conv2d(d_model, d_model, kernel_size=1)\n        # Activation after point-wise convolution\n        self.act2 = nn.GELU()\n        # Normalization after point-wise convolution\n        self.norm2 = nn.BatchNorm2d(d_model)\n\n    def forward(self, x: torch.Tensor):\n        # For the residual connection around the depth-wise convolution\n        residual = x\n\n        # Depth-wise convolution, activation and normalization\n        x = self.depth_wise_conv(x)\n        x = self.act1(x)\n        x = self.norm1(x)\n\n        # Add residual connection\n        x += residual\n\n        # Point-wise convolution, activation and normalization\n        x = self.point_wise_conv(x)\n        x = self.act2(x)\n        x = self.norm2(x)\n        #\n        return x\n\n\nclass PatchEmbeddings(Module):\n    \"\"\"\n    <a id=\"PatchEmbeddings\"></a>\n    ## Get patch embeddings\n    This splits the image into patches of size $p \\times p$ and gives an embedding for each patch.\n    \"\"\"\n\n    def __init__(self, d_model: int, patch_size: int, in_channels: int):\n        \"\"\"\n        * `d_model` is the number of channels in patch embeddings $h$\n        * `patch_size` is the size of the patch, $p$\n        * `in_channels` is the number of channels in the input image (3 for rgb)\n        \"\"\"\n        super().__init__()\n\n        # We create a convolution layer with a kernel size and and stride length equal to patch size.\n        # This is equivalent to splitting the image into patches and doing a linear\n        # transformation on each patch.\n        self.conv = nn.Conv2d(in_channels, d_model, kernel_size=patch_size, stride=patch_size)\n        # Activation function\n        self.act = nn.GELU()\n        # Batch normalization\n        self.norm = nn.BatchNorm2d(d_model)\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        * `x` is the input image of shape `[batch_size, channels, height, width]`\n        \"\"\"\n        # Apply convolution layer\n        x = self.conv(x)\n        # Activation and normalization\n        x = self.act(x)\n        x = self.norm(x)\n\n        #\n        return x\n\n\nclass ClassificationHead(Module):\n    \"\"\"\n    <a id=\"ClassificationHead\"></a>\n    ## Classification Head\n    They do average pooling (taking the mean of all patch embeddings) and a final linear transformation\n    to predict the log-probabilities of the image classes.\n    \"\"\"\n\n    def __init__(self, d_model: int):\n        \"\"\"\n        * `d_model` is the number of channels in patch embeddings, $h$\n        * `n_classes` is the number of classes in the classification task\n        \"\"\"\n        super().__init__()\n        # Average Pool\n        # self.pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.act = nn.GELU()\n        self.convtrans = nn.ConvTranspose2d(d_model, d_model, kernel_size=10, stride=8, padding=1)\n        self.batchnorm = nn.BatchNorm2d(d_model)\n        #这里尽量不要考虑上采用函数，因为这个线性插值的纯粹的数值计算是不能学习的，反卷积可以做到上采样\n        # self.upsample = nn.UpsamplingBilinear2d(scale_factor=2)\n        #1*1卷积这里，无论前面输出多好channel， 这里直接拿来作为输入就行了\n        self.adjust = nn.Conv2d(d_model, 2, kernel_size=1, stride=1, padding=0)\n        #由于目前用的交叉商函数自带softmax， 所以这里就不需要加入softmax了\n        # Linear layer\n        # self.linear = nn.Linear(d_model, n_classes)\n        # self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x: torch.Tensor):\n        # Average pooling\n        # x = self.pool(x)\n        x = self.act(x)\n        x = self.convtrans(x)\n        x = self.batchnorm(x)\n        # x = self.upsample(x)\n        # Get the embedding, `x` will have shape `[batch_size, d_model, 1, 1]`\n        # x = x[:, :, 0, 0]\n        # Linear layer\n        # x = self.linear(x)\n\n        # print(x)\n        # print('*'*25)\n        x = self.adjust(x)\n        # x = self.softmax(x)\n        # print(x.shape)\n        # print(x)\n\n        #\n        return x\n\n\nclass ConvMixer(Module):\n    \"\"\"\n    ## ConvMixer\n    This combines the patch embeddings block, a number of ConvMixer layers and a classification head.\n    \"\"\"\n    def __init__(self, conv_mixer_layer: ConvMixerLayer, n_layers: int,\n                 patch_emb: PatchEmbeddings,\n                 classification: ClassificationHead):\n        \"\"\"\n        * `conv_mixer_layer` is a copy of a single [ConvMixer layer](#ConvMixerLayer).\n         We make copies of it to make ConvMixer with `n_layers`.\n        * `n_layers` is the number of ConvMixer layers (or depth), $d$.\n        * `patch_emb` is the [patch embeddings layer](#PatchEmbeddings).\n        * `classification` is the [classification head](#ClassificationHead).\n        \"\"\"\n        super().__init__()\n        # Patch embeddings\n        self.patch_emb = patch_emb\n        # Classification head\n        self.classification = classification\n        # Make copies of the [ConvMixer layer](#ConvMixerLayer)\n        self.conv_mixer_layers = clone_module_list(conv_mixer_layer, n_layers)\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        * `x` is the input image of shape `[batch_size, channels, height, width]`\n        \"\"\"\n        # Get patch embeddings. This gives a tensor of shape `[batch_size, d_model, height / patch_size, width / patch_size]`.\n        x = self.patch_emb(x)\n\n        # Pass through [ConvMixer layers](#ConvMixerLayer)\n        for layer in self.conv_mixer_layers:\n            x = layer(x)\n\n        # Classification head, to get logits\n        x = self.classification(x)\n\n        #\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-12-19T14:36:55.523088Z","iopub.execute_input":"2021-12-19T14:36:55.523360Z","iopub.status.idle":"2021-12-19T14:36:55.543359Z","shell.execute_reply.started":"2021-12-19T14:36:55.523328Z","shell.execute_reply":"2021-12-19T14:36:55.542616Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2021-12-19T15:24:58.129882Z","iopub.execute_input":"2021-12-19T15:24:58.130455Z","iopub.status.idle":"2021-12-19T15:24:58.136529Z","shell.execute_reply.started":"2021-12-19T15:24:58.130416Z","shell.execute_reply":"2021-12-19T15:24:58.134652Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"df_all = pd.read_csv(TEST_CSV)\ntest_dataset = CellDataset_submit(TEST_PATH, df_all, patch_size=PATCH_SIZE)\n# batch_size must be 1 and shuffle must be False\ntest_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=2) \n","metadata":{"execution":{"iopub.status.busy":"2021-12-19T15:24:59.371785Z","iopub.execute_input":"2021-12-19T15:24:59.372327Z","iopub.status.idle":"2021-12-19T15:24:59.383265Z","shell.execute_reply.started":"2021-12-19T15:24:59.372271Z","shell.execute_reply":"2021-12-19T15:24:59.382432Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# Size of a patch, $p$\npatch_size: int = 8\n# Number of channels in patch embeddings, $h$\nd_model: int = 256\n# Number of [ConvMixer layers](#ConvMixerLayer) or depth, $d$\nn_layers: int = 20\n# Kernel size of the depth-wise convolution, $k$\nkernel_size: int = 7","metadata":{"execution":{"iopub.status.busy":"2021-12-19T15:25:01.771882Z","iopub.execute_input":"2021-12-19T15:25:01.772457Z","iopub.status.idle":"2021-12-19T15:25:01.776463Z","shell.execute_reply.started":"2021-12-19T15:25:01.772417Z","shell.execute_reply":"2021-12-19T15:25:01.775495Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"model = ConvMixer(ConvMixerLayer(d_model, kernel_size), n_layers,\n                      PatchEmbeddings(d_model, patch_size, 1),\n                      ClassificationHead(d_model))\n\nmodel.load_state_dict(torch.load(PRETRAIN_MODEL))\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2021-12-19T15:25:03.065574Z","iopub.execute_input":"2021-12-19T15:25:03.066182Z","iopub.status.idle":"2021-12-19T15:25:03.250581Z","shell.execute_reply.started":"2021-12-19T15:25:03.066144Z","shell.execute_reply":"2021-12-19T15:25:03.249784Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"## all predicted masks\npredicted_masks_list=np.zeros((len(df_all),576,704)) #576是为了拼64倍数\nnames = ['a' for i in range(len(df_all))]","metadata":{"execution":{"iopub.status.busy":"2021-12-19T15:39:14.859826Z","iopub.execute_input":"2021-12-19T15:39:14.860099Z","iopub.status.idle":"2021-12-19T15:39:14.866064Z","shell.execute_reply.started":"2021-12-19T15:39:14.860068Z","shell.execute_reply":"2021-12-19T15:39:14.864931Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"def merge_final8pixels(ls):\n    ls[:,512:520,:] = ls[:,-8:,:]\n    ls = ls[:,:520,:]\n    return ls","metadata":{"execution":{"iopub.status.busy":"2021-12-19T15:39:15.903994Z","iopub.execute_input":"2021-12-19T15:39:15.904624Z","iopub.status.idle":"2021-12-19T15:39:15.908994Z","shell.execute_reply.started":"2021-12-19T15:39:15.904580Z","shell.execute_reply":"2021-12-19T15:39:15.908203Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"patch_id = 0\nimg_id = 0\nrow_sums = HEIGHT // PATCH_SIZE +1\ncol_sums = WIDTH // PATCH_SIZE\npatch_nums = row_sums * col_sums\nmodel.eval()\nwith torch.no_grad():\n    for batch_idx, (X_batch,y_name) in enumerate(test_loader): \n        X_batch = Variable(X_batch.to(device))\n        y_out = model(X_batch)\n        y_out = F.softmax(y_out, dim=1)[:, 1:].squeeze(1)\n        \n        patch_id = batch_idx % patch_nums\n        img_id = batch_idx // patch_nums\n        \n        cur_r = patch_id // col_sums\n        cur_c = patch_id % col_sums\n\n        start_r = cur_r * PATCH_SIZE\n        end_r = start_r + PATCH_SIZE\n        start_c = cur_c * PATCH_SIZE\n        end_c = start_c + PATCH_SIZE\n        \n        predicted_masks_list[img_id, start_r:end_r, start_c:end_c] = y_out.cpu()\n        names[img_id] = y_name\n        \npredicted_masks_list = merge_final8pixels(predicted_masks_list)","metadata":{"execution":{"iopub.status.busy":"2021-12-19T15:39:17.082796Z","iopub.execute_input":"2021-12-19T15:39:17.083346Z","iopub.status.idle":"2021-12-19T15:39:19.849144Z","shell.execute_reply.started":"2021-12-19T15:39:17.083298Z","shell.execute_reply":"2021-12-19T15:39:19.848212Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.imshow(predicted_masks_list[0])\nplt.show","metadata":{"execution":{"iopub.status.busy":"2021-12-19T15:39:20.258249Z","iopub.execute_input":"2021-12-19T15:39:20.258832Z","iopub.status.idle":"2021-12-19T15:39:20.522820Z","shell.execute_reply.started":"2021-12-19T15:39:20.258796Z","shell.execute_reply":"2021-12-19T15:39:20.522063Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"markdown","source":"# predicting on test set is finish, waiting for next step","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"### test on validation data ","metadata":{}},{"cell_type":"code","source":"class IoUScore(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(IoUScore, self).__init__()\n\n    def forward(self, inputs, targets, smooth=1):\n\n\n\n        inputs = F.softmax(inputs, dim=1)[:, 1:]\n\n\n\n        #flatten label and prediction tensors\n        inputs = inputs.reshape(-1)\n        targets = targets.reshape(-1)\n\n        #intersection is equivalent to True Positive count\n        #union is the mutually inclusive area of all labels & predictions\n        intersection = (inputs * targets).sum()\n        total = (inputs + targets).sum()\n        union = total - intersection\n\n        IoU = (intersection + smooth)/(union + smooth)\n\n\n        return IoU.cpu().numpy()","metadata":{"execution":{"iopub.status.busy":"2021-12-19T15:45:35.730228Z","iopub.execute_input":"2021-12-19T15:45:35.730906Z","iopub.status.idle":"2021-12-19T15:45:35.737602Z","shell.execute_reply.started":"2021-12-19T15:45:35.730870Z","shell.execute_reply":"2021-12-19T15:45:35.736782Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"def rle_decode(mask_rle, shape):\n    \"\"\"\n    mask_rle: run-length as string formated (start length)\n    shape: (height,width) of array to return\n    Returns numpy array, 1 - mask, 0 - background\n    \"\"\"\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0] * shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape(shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-19T15:45:37.438864Z","iopub.execute_input":"2021-12-19T15:45:37.439340Z","iopub.status.idle":"2021-12-19T15:45:37.445000Z","shell.execute_reply.started":"2021-12-19T15:45:37.439296Z","shell.execute_reply":"2021-12-19T15:45:37.444349Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"class CellDataset(Dataset):\n    def __init__(self, image_dir, df, split='train', transforms=None, resize=False, patch_size=16):\n        self.transforms = transforms\n        self.image_dir = image_dir\n        self.df = df\n        self.patch_size = patch_size\n        self.split = split\n\n        self.should_resize = False\n        if self.should_resize:\n            self.height = int(HEIGHT * resize)\n            self.width = int(WIDTH * resize)\n        else:\n            self.height = HEIGHT\n            self.width = WIDTH\n\n        self.row_sum = self.height // self.patch_size +1\n        self.col_sum = self.width // self.patch_size\n        self.patch_num = self.row_sum * self.col_sum\n\n        self.image_info = collections.defaultdict(dict)\n        ##这一步将相同ID的annotation组成在一起。比如原文件关于id=001的annotation有400条(行），操作过后temp_df中id=001的len(annotation)=400（一行）.\n        temp_df_11 = self.df.groupby('id')['annotation'].agg(lambda x: list(x)).reset_index()\n\n        train_df, val_df = train_test_split(temp_df_11, test_size=0.2, random_state=42)\n        if self.split == 'train':\n            temp_df = train_df\n        elif self.split == 'val':\n            temp_df = val_df\n        print(len(temp_df))\n        ## 将id, path, annotation转成字典\n        ii = 0\n        for index, row in temp_df.iterrows():\n            self.image_info[ii] = {\n                'image_id': row['id'],\n                'image_path': os.path.join(self.image_dir, row['id'] + '.png'),\n                'annotations': row[\"annotation\"]\n            }\n            ii += 1\n\n    def __getitem__(self, idx):\n        ''' Get the image and the target'''\n        img_idx = idx // self.patch_num\n        patch_idx = idx % self.patch_num\n\n        img_path = self.image_info[img_idx][\"image_path\"]\n        img = Image.open(img_path)\n        img_name = self.image_info[img_idx]['image_id']\n\n        if self.should_resize:\n            img = img.resize((self.width, self.height), resample=Image.BILINEAR)\n\n        info = self.image_info[img_idx]\n\n        n_objects = len(info['annotations'])\n        # ********************************************************************************#\n        ##这一步得到的masks是将每一个a_mask放在不同的通道上，所有有多少a_mask就有多少通道\n        ## boxes记录着每个a_mask的边框的顶点\n        # masks = np.zeros((len(info['annotations']), self.height, self.width), dtype=np.uint8)\n        # boxes = []\n\n        # for i, annotation in enumerate(info['annotations']):\n        #     a_mask = rle_decode(annotation, (HEIGHT, WIDTH))\n        #     a_mask = Image.fromarray(a_mask)\n\n        #     if self.should_resize:\n        #         a_mask = a_mask.resize((self.width, self.height), resample=Image.BILINEAR)\n\n        #     a_mask = np.array(a_mask) > 0\n        #     masks[i, :, :] = a_mask\n\n        #     boxes.append(self.get_box(a_mask))\n        # ********************************************************************************#\n\n        # ********************************************************************************#\n        ##  这个写法可以让所有a_mask加在同一个通道上，看起来更正常点\n        masks = np.zeros((self.height, self.width))\n\n        for i, annotation in enumerate(info['annotations']):\n            a_mask = rle_decode(annotation, (HEIGHT, WIDTH))\n            a_mask = Image.fromarray(a_mask)\n\n            if self.should_resize:\n                a_mask = a_mask.resize((self.width, self.height), resample=Image.BILINEAR)\n\n            a_mask = np.array(a_mask) > 0\n            masks += a_mask\n\n        # ********************************************************************************#\n\n        masks = np.where(masks > 0, 1, 0)  # 大于0的地方取0，否则取1. 因为前面的a_mask在一些像素上重叠了，所以需要改成1\n        img = np.array(img)\n\n        # 计算patch位置\n        cur_row = patch_idx // self.col_sum\n        cur_col = patch_idx % self.col_sum\n\n        if cur_row == 8:\n            start_row = self.height - 64\n            end_row = start_row + self.patch_size\n            start_col = cur_col * self.patch_size\n            end_col = start_col + self.patch_size\n        else:\n            start_row = cur_row * self.patch_size\n            end_row = start_row + self.patch_size\n            start_col = cur_col * self.patch_size\n            end_col = start_col + self.patch_size\n        # 切片\n        mask_clip = masks[start_row:end_row, start_col:end_col]\n        image_clip = img[start_row:end_row, start_col:end_col]\n\n        transform = transforms.Compose([\n            transforms.ToTensor()\n        ])\n        image_clip = transform(image_clip)\n        mask_clip = transform(mask_clip)\n\n        mask_clip = torch.as_tensor(mask_clip, dtype=torch.long).squeeze(0)\n        image_clip = torch.as_tensor(image_clip, dtype=torch.float)\n\n        # if self.transforms is not None:\n        #     img, masks = self.transforms(img, masks)\n\n        return image_clip, mask_clip,img_name,masks\n\n    def __len__(self):\n        return len(self.image_info) * (520 // self.patch_size +1) * (704 // self.patch_size)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-19T15:45:39.127579Z","iopub.execute_input":"2021-12-19T15:45:39.127897Z","iopub.status.idle":"2021-12-19T15:45:39.164067Z","shell.execute_reply.started":"2021-12-19T15:45:39.127863Z","shell.execute_reply":"2021-12-19T15:45:39.163171Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nTRAIN_PATH = '../input/sartorius-cell-instance-segmentation/train'\nTRAIN_CSV='../input/sartorius-cell-instance-segmentation/train.csv'\ndf_all = pd.read_csv(TRAIN_CSV)\nval_dataset = CellDataset(TRAIN_PATH, df_all, patch_size=PATCH_SIZE, split='val')\nval_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=2)","metadata":{"execution":{"iopub.status.busy":"2021-12-19T15:45:43.075036Z","iopub.execute_input":"2021-12-19T15:45:43.075687Z","iopub.status.idle":"2021-12-19T15:45:43.404902Z","shell.execute_reply.started":"2021-12-19T15:45:43.075649Z","shell.execute_reply":"2021-12-19T15:45:43.404165Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"predicted_masks_list_val=np.zeros((len(df_all),576,WIDTH))\ntrue_masks_val = np.zeros((len(df_all),HEIGHT,WIDTH))\nnames_val = ['a' for i in range(len(df_all))]","metadata":{"execution":{"iopub.status.busy":"2021-12-19T15:45:44.807735Z","iopub.execute_input":"2021-12-19T15:45:44.808454Z","iopub.status.idle":"2021-12-19T15:45:44.816651Z","shell.execute_reply.started":"2021-12-19T15:45:44.808418Z","shell.execute_reply":"2021-12-19T15:45:44.815902Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"patch_id = 0\nimg_id = 0\nrow_sums = HEIGHT // PATCH_SIZE +1\ncol_sums = WIDTH // PATCH_SIZE\npatch_nums = row_sums * col_sums\n\nmodel.eval()\niou_list = list()\nmetric = IoUScore()\nwith torch.no_grad():\n    # for batch_idx, (X_batch, y_batch, *rest) in enumerate(val_loader): #Wang Yu\n    for batch_idx, (X_batch, y_batch, y_name,true_mask) in enumerate(val_loader):  # Guan Hui\n        X_batch = Variable(X_batch.to(device='cuda'))\n        y_batch = Variable(y_batch.to(device='cuda'))\n        # start = timeit.default_timer()\n        y_out = model(X_batch)\n#         iou_score = metric(y_out, y_batch)\n#         iou_list.append(iou_score)\n        y_out = F.softmax(y_out, dim=1)[:, 1:].squeeze(1)\n        \n        patch_id = batch_idx % patch_nums\n        img_id = batch_idx // patch_nums\n        \n        cur_r = patch_id // col_sums\n        cur_c = patch_id % col_sums\n\n        start_r = cur_r * PATCH_SIZE\n        end_r = start_r + PATCH_SIZE\n        start_c = cur_c * PATCH_SIZE\n        end_c = start_c + PATCH_SIZE\n        \n        predicted_masks_list_val[img_id, start_r:end_r, start_c:end_c] = y_out.cpu()\n        names_val[img_id] = y_name\n        true_masks_val[img_id] = true_mask\n        \n        if img_id >25:\n            break\n\n#     avg_iou = np.mean(iou_list)\n#     print(\"current epoch:  current mean iou: {:.4f}\".format(avg_iou))\n\n    model.train()\npredicted_masks_list_val = merge_final8pixels(predicted_masks_list_val)","metadata":{"execution":{"iopub.status.busy":"2021-12-19T15:45:47.171361Z","iopub.execute_input":"2021-12-19T15:45:47.171898Z","iopub.status.idle":"2021-12-19T15:47:54.334328Z","shell.execute_reply.started":"2021-12-19T15:45:47.171858Z","shell.execute_reply":"2021-12-19T15:47:54.333341Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.imshow(predicted_masks_list_val[5])\nplt.show","metadata":{"execution":{"iopub.status.busy":"2021-12-19T15:51:03.241707Z","iopub.execute_input":"2021-12-19T15:51:03.242145Z","iopub.status.idle":"2021-12-19T15:51:03.491032Z","shell.execute_reply.started":"2021-12-19T15:51:03.242105Z","shell.execute_reply":"2021-12-19T15:51:03.490376Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"plt.imshow(true_masks_val[5])\nplt.show","metadata":{"execution":{"iopub.status.busy":"2021-12-19T15:51:06.356199Z","iopub.execute_input":"2021-12-19T15:51:06.356808Z","iopub.status.idle":"2021-12-19T15:51:06.602835Z","shell.execute_reply.started":"2021-12-19T15:51:06.356764Z","shell.execute_reply":"2021-12-19T15:51:06.602126Z"},"trusted":true},"execution_count":82,"outputs":[]}]}